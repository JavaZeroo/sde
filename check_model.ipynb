{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljb/miniconda3/envs/sde/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    in_channels=3,\n",
    "    encoder_depth=3, \n",
    "    decoder_channels=(64, 32, 16),\n",
    "    classes=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "class timeUnetPlusPlus(smp.UnetPlusPlus):\n",
    "    def __init__(self, model_channels, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # self.timeEmb\n",
    "        self.model_channels = model_channels\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "        self.lns = nn.ModuleList()\n",
    "        self.lns.append(nn.Linear(time_embed_dim, 2))\n",
    "        self.lns.append(nn.Linear(time_embed_dim, 32))\n",
    "        self.lns.append(nn.Linear(time_embed_dim, 24))\n",
    "        self.lns.append(nn.Linear(time_embed_dim, 40))\n",
    "        \n",
    "        self.name = 'timeUnetPlusPlus'\n",
    "        self.initialize()\n",
    "\n",
    "    def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                        These may be fractional.\n",
    "        :param dim: the dimension of the output.\n",
    "        :param max_period: controls the minimum frequency of the embeddings.\n",
    "        :return: an [N x dim] Tensor of positional embeddings.\n",
    "        \"\"\"\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "    def forward(self, x, timesteps):        \n",
    "        emb = self.time_embed(self.timestep_embedding(timesteps, self.model_channels))\n",
    "        print(emb.shape)\n",
    "        self.check_input_shape(x)\n",
    "        features = self.encoder(x)\n",
    "        for index, f in enumerate(features):\n",
    "            temp_emb = self.lns[index](emb)\n",
    "            while len(temp_emb.shape) < len(f.shape):\n",
    "                temp_emb = temp_emb[..., None]\n",
    "            print(f.shape, temp_emb.shape)\n",
    "            f = f + temp_emb\n",
    "        decoder_output = self.decoder(*features)\n",
    "\n",
    "        masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "        if self.classification_head is not None:\n",
    "            labels = self.classification_head(features[-1])\n",
    "            return masks, labels\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    \n",
    "model = timeUnetPlusPlus(\n",
    "    model_channels=64,\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    in_channels=2,\n",
    "    encoder_depth=3, \n",
    "    decoder_channels=(64, 32, 16),\n",
    "    classes=1,\n",
    "    ).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (dtype=torch.dtype, end=Tensor, start=int, ), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m100\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model(x, t)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mtimeUnetPlusPlus.forward\u001b[0;34m(self, x, timesteps)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, timesteps):        \n\u001b[0;32m---> 49\u001b[0m     emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimestep_embedding(timesteps, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_channels))\n\u001b[1;32m     50\u001b[0m     \u001b[39mprint\u001b[39m(emb\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input_shape(x)\n",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m, in \u001b[0;36mtimeUnetPlusPlus.timestep_embedding\u001b[0;34m(timesteps, dim, max_period)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mCreate sinusoidal timestep embeddings.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m:param timesteps: a 1-D Tensor of N indices, one per batch element.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m:return: an [N x dim] Tensor of positional embeddings.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m half \u001b[39m=\u001b[39m dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     38\u001b[0m freqs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\n\u001b[0;32m---> 39\u001b[0m     \u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39mlog(max_period) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49marange(start\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, end\u001b[39m=\u001b[39;49mhalf, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32) \u001b[39m/\u001b[39m half\n\u001b[1;32m     40\u001b[0m )\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mtimesteps\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     41\u001b[0m args \u001b[39m=\u001b[39m timesteps[:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mfloat() \u001b[39m*\u001b[39m freqs[\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     42\u001b[0m embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mcos(args), torch\u001b[39m.\u001b[39msin(args)], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (dtype=torch.dtype, end=Tensor, start=int, ), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(100, 2, 32, 32).to('cuda')\n",
    "t = torch.rand(100).to('cuda')\n",
    "model(x, t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
